# Dynamic Few-Shot Examples in Chat Models
## Replace the (multiple types of) examples passed into FewShotChatMessagePromptTemplate with an example_selector
## Before install Chroma, make sure C++ environment is established
### Install with <https://visualstudio.microsoft.com/zh-hant/visual-cpp-build-tools/>

import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings
from langchain_core.prompts import FewShotChatMessagePromptTemplate


# Change false to true when needed
os.environ["LANGSMITH_TRACING"] = "true" 
LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# Openai api with Langchain Framework
# setup model's parameter, like Temperature, N, top_p, etc
llm = ChatOpenAI(
    model = "gpt-4-turbo",
    temperature = 1.2,
)

prompt_template = ChatPromptTemplate.from_messages(
    [
        ("user", "{input}"),
        ("system", "{output}"),
    ]
)

few_shot_examples = [
    # Examples type 1 --- Math Question
    {"input": "2 ğŸ¦œ 2", "output": "4"},
    {"input": "2 ğŸ¦œ 3", "output": "5"},
    {"input": "2 ğŸ¦œ 4", "output": "6"},

    # Examples type 2 --- QA Example
    {"input": "What did the cow say to the moon?", "output": "nothing at all"},
    {"input": "How many legs does the horses have?", "output": "horses have 4 legs"},
    
    # Examples type 3 --- Poetry Writing
    {
        "input": "Write me a poem about the moon",
        "output": "One for the moon, and one for me, who are we to talk about the moon?",
    },
]
# å°‡æ–‡å­—å‘é‡åŒ–
## å°‡ç¯„ä¾‹ç”¨ç©ºæ ¼ä¸²æˆå­—ä¸²ï¼Œä¾› Embedding ä½¿ç”¨
to_vectorize = [" ".join(example.values()) for example in few_shot_examples]
## å»ºç«‹ä¸€å€‹ Embedding Object(ç‰©ä»¶) â†’ æŠŠæ–‡å­—è½‰æˆå‘é‡
embeddings = OpenAIEmbeddings()
## åˆ©ç”¨ "Chroma.from_texts" å»ºç«‹å‘é‡è³‡æ–™åº«
vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=few_shot_examples)

# åˆ©ç”¨ "SemanticSimilarityExampleSelector"  æ‰¾å‡ºèˆ‡ Prompt ç›¸ä¼¼åº¦æœ€é«˜çš„ few-shot ç¯„ä¾‹
## ä¸æœƒç›´æ¥ç”¢å‡º final_prompt, å‘¼å« llm 
example_selector = SemanticSimilarityExampleSelector(
    vectorstore = vectorstore,
    k = 2, ## å° Promptï¼Œåªé¸å‡ºå‰ 2 å€‹æœ€ç›¸ä¼¼çš„ç¯„ä¾‹
)
## check which examples will pick with user inputs
''' Generation Part
selected = example_selector.select_examples({"input": "horse"})
print(selected)
'''

# Define Few-Shot Prompt Template
## Must to combine ChatPromptTemplate or chain to make final prompt
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt = prompt_template,
    example_selector = example_selector,
    # è‹¥ä½¿ç”¨ static few-shot model å‰‡å°‡ example_selector æ”¹ç‚º examples
    # examples = few_shot_examples, 
)

# Combine to make final prompt
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are genious, you can answer everything."),
        few_shot_prompt,
        ("user", "{input}"),
    ]
)

# chain = final_prompt | llm_2 æ˜¯ä¸€ç¨® pipeline çš„æ¦‚å¿µï¼Œå°‡ final_prompt çš„è¼¸å‡ºç›´æ¥æ¥åˆ° llm_2
# final_prompt è² è²¬ï¼šæŠŠ system è¨Šæ¯ã€few-shot examplesã€user çš„ input ä¸€èµ·çµ„åˆæˆæœ€çµ‚å®Œæ•´çš„ Chat è¨Šæ¯åˆ—è¡¨ã€‚
chain = final_prompt | llm

# Different Results between normal text response and few-shot 
response = chain.invoke({"input": "Kangaroo"})
print(response.content)

''' éŒ¯èª¤ Generation Example
æ­¤æ–¹æ³•æ¨¡å‹ç„¡æ³•ç†è§£ few-shot examples
response = llm.invoke({"input": "Kangaroo"})
print(response.content)
'''