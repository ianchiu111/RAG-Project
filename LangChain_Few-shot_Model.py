# Dynamic Few-Shot Examples in Chat Models
## Replace the (multiple types of) examples passed into FewShotChatMessagePromptTemplate with an example_selector
## Before install Chroma, make sure C++ environment is established
### Install with <https://visualstudio.microsoft.com/zh-hant/visual-cpp-build-tools/>

import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings
from langchain_core.prompts import FewShotChatMessagePromptTemplate


# Change false to true when needed
os.environ["LANGSMITH_TRACING"] = "true" 
LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# Openai api with Langchain Framework
# setup model's parameter, like Temperature, N, top_p, etc
llm = ChatOpenAI(
    model = "gpt-4-turbo",
    temperature = 1.2,
)

prompt_template = ChatPromptTemplate.from_messages(
    [
        ("user", "{input}"),
        ("system", "{output}"),
    ]
)

few_shot_examples = [
    # Examples type 1 --- Math Question
    {"input": "2 ğŸ¦œ 2", "output": "4"},
    {"input": "2 ğŸ¦œ 3", "output": "5"},
    {"input": "2 ğŸ¦œ 4", "output": "6"},

    # Examples type 2 --- QA Example
    {"input": "What did the cow say to the moon?", "output": "nothing at all"},
    {"input": "How many legs does the horses have?", "output": "horses have 4 legs"},
    
    # Examples type 3 --- Poetry Writing
    {
        "input": "Write me a poem about the moon",
        "output": "One for the moon, and one for me, who are we to talk about the moon?",
    },
]
# å°‡æ–‡å­—å‘é‡åŒ–
## å°‡ç¯„ä¾‹ç”¨ç©ºæ ¼ä¸²æˆå­—ä¸²ï¼Œä¾› Embedding ä½¿ç”¨
to_vectorize = [" ".join(example.values()) for example in few_shot_examples]
## å»ºç«‹ä¸€å€‹ Embedding Object(ç‰©ä»¶) â†’ æŠŠæ–‡å­—è½‰æˆå‘é‡
embeddings = OpenAIEmbeddings()
## åˆ©ç”¨ "Chroma.from_texts" å»ºç«‹å‘é‡è³‡æ–™åº«
vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=few_shot_examples)

# åˆ©ç”¨ "SemanticSimilarityExampleSelector"  æ‰¾å‡ºèˆ‡ Prompt ç›¸ä¼¼åº¦æœ€é«˜çš„ few-shot ç¯„ä¾‹
## ä¸æœƒç›´æ¥ç”¢å‡º final_prompt, å‘¼å« llm 
example_selector_ï¼‘ = SemanticSimilarityExampleSelector(
    vectorstore = vectorstore,
    k = 2, ## å° Promptï¼Œåªé¸å‡ºå‰ 2 å€‹æœ€ç›¸ä¼¼çš„ç¯„ä¾‹
)
## check which examples will pick with user inputs
''' Generation Part
selected = example_selector.select_examples({"input": "horse"})
print(selected)
'''

# Define Few-Shot Prompt Template
## Must to combine ChatPromptTemplate or chain to make final prompt
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt = prompt_template,
    example_selector = example_selector_ï¼‘,
    # è‹¥ä½¿ç”¨ static few-shot model å‰‡å°‡ example_selector æ”¹ç‚º examples
    # examples = few_shot_examples, 
)

# Combine to make final prompt
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are genious, you can answer everything."),
        few_shot_prompt,
        ("user", "{input}"),
    ]
)

# chain = final_prompt | llm_2 æ˜¯ä¸€ç¨® pipeline çš„æ¦‚å¿µï¼Œå°‡ final_prompt çš„è¼¸å‡ºç›´æ¥æ¥åˆ° llm_2
# final_prompt è² è²¬ï¼šæŠŠ system è¨Šæ¯ã€few-shot examplesã€user çš„ input ä¸€èµ·çµ„åˆæˆæœ€çµ‚å®Œæ•´çš„ Chat è¨Šæ¯åˆ—è¡¨ã€‚
chain = final_prompt | llm

''' Generation Part
# Different Results between normal text response and few-shot 
response = chain.invoke({"input": "Kangaroo"})
print(response.content)
'''


''' éŒ¯èª¤ Generation Example
æ­¤æ–¹æ³•æ¨¡å‹ç„¡æ³•ç†è§£ few-shot examples
response = llm.invoke({"input": "Kangaroo"})
print(response.content)
'''


# åˆ©ç”¨è‡ªå®šç¾©çš„æ–¹å¼å»ºç«‹ Example Slector
from langchain_core.example_selectors.base import BaseExampleSelector

# CustomExampleSelector ç¹¼æ‰¿ BaseExampleSelector
## åˆ©ç”¨ prompt length æ¯”è¼ƒ prompt èˆ‡ example çš„é—œè¯æ€§ â†’ æ•ˆæœæ‡‰è¼ƒå·®
class Length_ExampleSelector(BaseExampleSelector):
    # Initialize the class
    ## selfï¼šè¡¨ç¤º object æœ¬èº«
    ## exampleï¼šè¡¨ç¤ºå‘¼å«çš„åƒæ•¸
    def __init__(self, examples):
        # é€é self.example å°‡å‚³å…¥çš„ examples å‚æ•°èµ‹å€¼ç»™ object çš„ examples å±¬æ€§
        self.examples = examples

    # define the add function
    ## åŠ å…¥æ–°è¼¸å…¥çš„ example
    def add_example(self, example):
        self.examples.append(example)

    # define the select functioin
    ## å°‹æ‰¾èˆ‡ä½¿ç”¨è€… prompt é•·åº¦ç›¸åŒçš„ example
    def select_examples(self, input_variables):
        # This assumes knowledge that part of the input will be a 'text' key
        new_word = input_variables["input"]
        new_word_length = len(new_word)

        # Initialize variables to store the best match and its length difference
        best_match = None
        smallest_diff = float("inf")

        # Iterate through each example
        for example in self.examples:
            # è¨ˆç®— promptã€example ä¹‹é–“çš„å­—æ•¸å·®è·
            current_diff = abs(len(example["input"]) - new_word_length)

            # æ›´æ–°æœ€ç›¸è¿‘çš„ example
            if current_diff < smallest_diff:
                smallest_diff = current_diff
                best_match = example

        return [best_match]

example_selector_2 = Length_ExampleSelector(few_shot_examples)
# æ¸¬è©¦ prompt å°æ‡‰åˆ°çš„ example ç‚ºä½•
print(example_selector_2.select_examples({"input": "okay"}))

example_selector_2.add_example({"input": "hand", "output": "mano"})
print(example_selector_2.select_examples({"input": "okay"}))


''' Generation Partï¼šsame way to generate with pipeline and example_selector
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt = prompt_template,
    example_selector = example_selector_2,
)    
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are genious, you can answer everything."),
        few_shot_prompt,
        ("user", "{input}"),
    ]
)
chain = final_prompt | llm
response = chain.invoke({"input": "Kangaroo"})
print(response.content)
'''






