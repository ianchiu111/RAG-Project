'''
To Build a Chat Model
'''

# Topic 1：function / tool Calling
1. 使用 tool calling 需要結合 Agent 代理功能

2. @tool decorator 
    a. 在 LangChain 中，@tool 提供了一種簡潔的方法來將函式轉換為工具，並自動處理名稱、描述和參數等屬性
    b. 不使用 @tool 裝飾器時，則需要手動定義工具的各個屬性，可能導致潛在的錯誤。以 @tool 自動處理細節

3. Tool 包含以下所有內容
    a. functioin name
    b. type hints
    c. docstring(文件字串)

4. Example：
    @tool
    def add(a: int, b: int) -> int:
    """ 兩數字相加 """
    return a + b

5. 可結合 "Agent" 超級好用!!!!!!!
    a. Example：
    ''' Code Block
    tools = [add, multiply]

    agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.OPENAI_FUNCTIONS,  # Choose appropriate agent type
    verbose=True  # Set to True for detailed logs
    )

    query = "What is 2 + 9?"
    response = agent.run(query)
    print(f"Response: {response}")
    '''

    b. Reference for Agent Type：https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_types.AgentType.html

6. Other Reference
    a. https://langchain-ai.github.io/langgraph/how-tos/tool-calling/#define-tools    

7. 使用 Agent 生成 v.s. 直接調用 LLM
    a. Agent：
        (1) 可使用多種工具（tools）來增強功能
        (2) 可指定 Agent 的類型
        (3) verbose 可輸出詳細的日誌，增加模型解釋性
        (4) 使用 Agent 花費高，約為 LLM 10倍以上
        (5) 適合後期需要詳細的運行日誌來監控和調整

    b. LLM：    
        (1) 功能性較差，容易限制於 LLM 本身
        (2) 需手動擴展功能
        (3) 成本低，回應快速，開發較方便
        (4) 適合早期迭代與驗證使用

# Topic 2：Cache Chat Model Response
1. reduce cost of calling API if the user frequently request same question

2. something needs（無看見特別功效）
    a. from langchain_core.globals import set_llm_cache
    b. from langchain_core.caches import InMemoryCache
    c. set_llm_cache(InMemoryCache())

